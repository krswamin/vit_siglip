{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40e6af4-d52b-4ba1-96e4-2a667dd1a87c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39872a51-3bc5-42ad-ae69-6858510f7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8add6bd8-b2f4-462f-a006-10e6d90d608c",
   "metadata": {},
   "source": [
    "## Attention Formula , Single Head of Attention, Multi Head Attention\n",
    "See Readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebddcec-ea13-41f8-9c55-2893be990d86",
   "metadata": {},
   "source": [
    "## Single Head Attention, Multi Head Attention\n",
    "### Precursors to SiglipAttention\n",
    "Simple introductory implementation of Single Head Attention, and Multi Head Attention that serve as a precursor to SiglipAttention. \n",
    "\n",
    "Note: This is not how Hugging face implements it. To be able to use Hugging Face weights, we would have to make use of vectorization and implement it parallely. This will be done in the subsequent section of SiglipAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bd1e4-07db-491c-a5d0-38478675aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Single Head Attention: A single head of the multi head attention module \"\"\"\n",
    "    def __init__(self, n_in, head_size, context_length):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        # The linear layers whose outputs will result in query, key, value respectively\n",
    "        self.query_linear = nn.Linear(n_in, head_size, bias=False)\n",
    "        self.key_linear   = nn.Linear(n_in, head_size, bias=False)\n",
    "        self.value_linear = nn.Linear(n_in, head_size, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Note B, T, C is not used here. But it is used in the SiglipAttention class\n",
    "        # B:batch_size,\n",
    "        # T:num_tokens (same as number of embeddings = 196) \n",
    "        # C: embedding_size=hidden_size = 768\n",
    "        B, T, C = x.shape\n",
    "        # Q, K, V       \n",
    "        q = self.query_linear(x)\n",
    "        k = self.key_linear(x)\n",
    "        v = self.value_linear(x)\n",
    "\n",
    "        # The scale dk\n",
    "        dk = self.head_size\n",
    "        # Attention-Filter = Q*Ktranspose\n",
    "        attn = q@k.transpose(-2,-1)\n",
    "        # The rest of the operations to calculate the final attention\n",
    "        attn = attn/math.sqrt(dk)\n",
    "        attn = F.softmax(attn, dim =-1)\n",
    "        attn = attn @ v\n",
    "        return attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi Head Attention: Concatenate outputs of several single head attention modules \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_heads, n_in, head_size, context_length):\n",
    "        super().__init__()\n",
    "        # Indvidual Single Attention Heads\n",
    "        self.attn_heads = [Head(n_in, head_size, context_length) for _ in range(num_of_heads)]\n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        self.out_proj  = nn.Linear(n_in, n_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Indvidual Single Attention Heads\n",
    "        out = [h(x) for h in self.attn_heads]\n",
    "        # Concatenate the individual attention heads\n",
    "        out = torch.concat(out, -1)\n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        # Here you need to project it back to the size of the incoming hidden_states/ residual \n",
    "        # to be able to add and create a residual\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b032a5-bf34-4689-b618-4ca7d4e977f0",
   "metadata": {},
   "source": [
    "## SiglipVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535cad2-af5c-4347-a475-43ab65aed9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@dataclass is a decorator in Python that automatically generates special methods like __init__, __repr__, and __eq__ for a class. \n",
    "This simplifies the creation of classes primarily used for storing data. \n",
    "It reduces boilerplate code and improves readability, especially when dealing with objects that mainly hold data.\n",
    "'''\n",
    "@dataclass\n",
    "class SiglipVisionConfig:   \n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    hidden_size: int = 768\n",
    "\n",
    "    num_channels: int = 3\n",
    "    num_attention_heads: int = 12\n",
    "    attention_dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df134c60-067a-4019-9c47-708d30b17e17",
   "metadata": {},
   "source": [
    "## SiglipAttention\n",
    "- Vectorized implementation of Multi head attention. Same implementation as Hugging Face (different from the non vectorized implementation above)\n",
    "- You don't have Single Attention Heads. You process all single attention heads parallely\n",
    "- This is more memory efficient\n",
    "- This enables using Hugging Face's pretrained weights in our model\n",
    "\n",
    "#### Vectorized Implementation\n",
    "In the vectorized implementation there might seem to be a lot of transposes, shape changes etc. \\\n",
    "Here is a high leve overview of it. \n",
    "- i)   q, k, v_states are the same dimensions as hidden_states = [1,196,768] = [batch, num_patches, embedding_dimension] \n",
    "- ii)  split q,k,v_states into 12 attention heads, along the embedding_dimension (768)  = [1,196,12,64]\n",
    "- iii) transpose q,k, v_states so that its [batch, num_heads] first and then [196,64] for vectorized multiplication\n",
    "- iv)  attention filter = q*k_transpose is a square: [1,12, 196,196]\n",
    "- v)   scaled attention filter, softmax, dropout are all the same square = [1,12,196,196]\n",
    "- vi)  mutltiply attention_weights*v and the dimesion is back to : [ 1, 12, 196,64]\n",
    "- vii) transpose attention again so that [12,64] the embeddind dimensions are back together for the concatenation: [ 1,196,12,64]\n",
    "- viii) Concatenate: merge 12 attention heads to get back the embedding dimension 768=12*64: [1. 196, 768]\n",
    "- ix)   Project back to residual states shape(which happens to be the same): [1,196,768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5cea0-746d-4f3e-ab13-c3274e51eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.dropout = config.attention_dropout\n",
    "        \n",
    "        # The linear layers whose outputs will result in query, key, value respectively\n",
    "        # This is just one unified set of projection heads across the the multi head attention module\n",
    "        # i.e there arent multiple projection layers defined for the single attention heads\n",
    "        # The output of this will be reshaped (see def forward). This is more memory efficient and\n",
    "        # enables the use of hugging face weights\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)   \n",
    "        \n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    # Note: the hidden states are the embeddings\n",
    "    def forward(self, hidden_states):\n",
    "        # the hidden states are the embeddings of the patches, so (batch_size, num_patches, embed_dim)\n",
    "        # Here B: batch_size = 1, T: num_tokens=num_patches = 196, C: embedding_dimension = 768\n",
    "        B, T, C = hidden_states.shape\n",
    "        print(\"\\n-----------SiglipAttention: forward details-------------\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Notice the dimensions at each stage to understand the vectorized implementation of multi head attention\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"B (batch)              :\", B)\n",
    "        print(\"T (#tokens = #patches) :\", T)\n",
    "        print(\"C (embed_dim)          :\", C)\n",
    "        \n",
    "        # Query, Key and Values\n",
    "        # These are the q, k,v values across all attention heads. There are 3 large vectors\n",
    "        # vectorized dot-product(mat-mul) of hidden_states =[1, 196, 768] & q_proj = [768,768]\n",
    "        # q,k,v_states: shape = [196x768]*[768x768] = [ 196x768] across all batches = [1,196,768]\n",
    "        q_states = self.q_proj(hidden_states)        \n",
    "        k_states = self.k_proj(hidden_states)    \n",
    "        v_states = self.v_proj(hidden_states)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states, k_states, v_states: output of the linear layers\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states.shape =\", q_states.shape)\n",
    "        print(\"k_states.shape =\", k_states.shape)\n",
    "        print(\"v_states.shape =\", v_states.shape) \n",
    "        \n",
    "        # Reshape Multi Head Attention into 12 units        \n",
    "        # Divide the Q, K, V vectors across all attention heads. This is what C // self.num_attention_heads does\n",
    "        # We split along the embedding dimension 768 (and not along the number of patches = 196)\n",
    "        # Hence qualitatively this is like splitting the large embedding vector into smaller pieces corresponding to the 12 individual heads\n",
    "        # This step is what we do differently: We do not concatenate anymore. \n",
    "        # i.e. in the previous step it was already concatenated and at 768. In this step we split 768 into 12*64 \n",
    "        # so q,k,v_states: ([1, 196, 768]) --> becomes torch.Size([1, 196, 12, 64])\n",
    "        q_states = q_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "        k_states = k_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "        v_states = v_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states, k_states, v_states: After splitting 768 to 12 attention heads\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states.shape =\", q_states.shape)\n",
    "        print(\"k_states.shape =\", k_states.shape)\n",
    "        print(\"v_states.shape =\", v_states.shape) \n",
    "\n",
    "        # Transpose the states so that the dot product dimensions i,e [196,64] are the last 2 dimensions.\n",
    "        # first 2 dimensions are batch_size and num_heads. This enables easy vectorized mat-mul      \n",
    "        # q_states.transpose(1, 2) swaps dimension-index-1 with dimension-index-2 i.e. 2nd and 3rd dimension\n",
    "        # so q,k,v_states [1,196,12,64] ---> become [1,12,196,64]  \n",
    "        q_states = q_states.transpose(1, 2)\n",
    "        k_states = k_states.transpose(1, 2)\n",
    "        v_states = v_states.transpose(1, 2)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states, k_states, v_states: transpose for easy vectorized dotproduct(mat-mul)\")\n",
    "        print(\"so that first 2 dimensions are [batch_size, num_heads] = [1,12]\")\n",
    "        print(\"last 2 dimensions are [num_patches, embed_dim/num_heads]= [196,64]\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"q_states.shape =\", q_states.shape)\n",
    "        print(\"k_states.shape =\", k_states.shape)\n",
    "        print(\"v_states.shape =\", v_states.shape) \n",
    "\n",
    "        # The scale dk = 64\n",
    "        dk = k_states.size(-1)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Scale is the embedding dimension per single attention head = 768/12 = 64\")\n",
    "        print(\"Note: in the dk scale diagram in the Readme.md (section 5.4) , \\\n",
    "        \\n - you multiply [500,50] x[50,500] and the scale dk = 500. \\\n",
    "        \\n   dk = 500 seems to be #of patches instead of the embedding size .(This could have been a mistake) \\\n",
    "        \\n - But here in the code, you multiply [196,64] x[64,196] for the attention filter. \\\n",
    "        \\n  dk = 64 = embedding dimension. I think embedding dimension makes more sense\" )\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"dk: scale = \", dk)\n",
    "\n",
    "        # Attention-Filter = Q*Ktranspose (this is a square)\n",
    "        # k_states.transpose(-2, -1) swaps dimension-index-2 with dimension-index-1 i.e. last two dimensions\n",
    "        # vectorized dot-product(mat-mul) of q=[1,12,196,64] & k_transpose = [1,12,64,196]\n",
    "        # attn: shape = [196x64]*[64x196] = [ 196x196] across all 12 heads = [1,12,196,196]       \n",
    "        attn = q_states @ k_states.transpose(-2, -1)\n",
    "        \n",
    "        # Scaled attention filter: attn shape = = [1,12,196,196]\n",
    "        attn = attn/math.sqrt(dk)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\" In all the below cases attn: shape is a square i.e 196*196)\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"i)  attn.shape: scaled attention filter    : \", attn.shape)\n",
    "\n",
    "        # Softmax rowwise to get probability distribution: attn shape = = [1,12,196,196]\n",
    "        attn = F.softmax(attn, dim=-1).to(q_states.dtype)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Apply softmax. Since attn is [196, 196] it makes no sense to normalize for the entire square \\n \\\n",
    "        Apply softmax to get probability distribution along dimension -1 which means across the columns i.e. rowwise\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"ii)  attn.shape: after softmax              : \", attn.shape)\n",
    "\n",
    "        # Droput : attn shape = = [1,12,196,196]\n",
    "        # Why is dropout being applied before multiplication by v_states\n",
    "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "        print(\"iii) attn.shape: after dropout              : \", attn.shape)\n",
    "\n",
    "        # Weighted Sum: allows information flow between tokens(patches)\n",
    "        # vectorized dot-product(mat-mul) of attn=[1,12,196,196] & v_states = [1,12,196,64]\n",
    "        # attn: shape = [196x196]*[196x64] = [ 196x64] across all 12 heads = [1,12,196,64]\n",
    "        attn = attn @ v_states\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Multiply attn with v_states. Attention is back to [196,64]\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"iv) attn.shape: mult with v_states         : \", attn.shape)\n",
    "\n",
    "        # Transpose it back to the original q_states view where [12,64] are the end\n",
    "        # so that they can be fused to make the original 12*64 = 768\n",
    "        # attn.transpose(1, 2) swaps dimension-index-1 with dimension-index-2 i.e. 2nd and 3rd dimension\n",
    "        # attn [1,12,196,64] ---> becomes [1,196,12,64]\n",
    "        attn = attn.transpose(1, 2)\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Transpose it back to the original q_states view where [12,64] are the end\")\n",
    "        print(\"[12,64] can be easily multiplied to recover the original embed_dim = 768\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"v) attn.shape: after transpose            : \", attn.shape)\n",
    "\n",
    "        # i) This reshaping concatenates the 12*64 outputs from 12 attention heads back to the embedding_dim = 768\n",
    "        # ii) Operations like transpose can make memory layout very inefficient.\n",
    "        #     Make sure that the vector is contiguous in memory for efficient implementation\n",
    "        # attn [1,196,12,64] ---> becomes [1,196,768]\n",
    "        attn = attn.reshape(B, T, C).contiguous()\n",
    "        print(\"\\n\\n-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Concatenate 12 attention heads so that [12x64] = 768\")\n",
    "        print(\"-------------------------------------------------------------------------------------------------\")\n",
    "        print(\"vi) attn.shape: after reshaping to B, T, C : \", attn.shape)\n",
    "        \n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        # vectorized dot-product(mat-mul) of attn =[1, 196, 768] & out_proj = [768,768]\n",
    "        # attn: shape = [196x768]*[768x768] = [ 196x768] across all batches = [1,196,768]\n",
    "        attn = self.out_proj(attn)\n",
    "        print(\"vii) attn.shape: after out_proj             : \", attn.shape)\n",
    "        print(\"\\n\\n------- End of SiglipAttention: Forward -------------- \\n\")\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34879b9-70fc-4ebe-9065-ba92352fb061",
   "metadata": {},
   "source": [
    "## Create random Hidden State/ Embeddings and pass through Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8890a1-5671-4b21-b250-6b98bf1740c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_patches = 196\n",
    "hidden_size = 768 # same as embedding dimension\n",
    "attention_dropout=0.0\n",
    "num_attention_heads=12\n",
    "hidden_size=768\n",
    "\n",
    "hidden_states = torch.randn(batch_size, num_patches, hidden_size)\n",
    "config = SiglipVisionConfig(\n",
    "    attention_dropout=attention_dropout,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "attention = SiglipAttention(config)\n",
    "\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba18cf-a8aa-4b5a-9c50-d934502e8d47",
   "metadata": {},
   "source": [
    "### Sanity Check #1\n",
    "The input shape and output shape match i.e the incoming hidden states and the output hidden states/ residual should have the same shape by design. See Siglip Transformer Encoder Atrchitecture for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e84401-8c32-442c-8f35-c73e501ae269",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = attention(hidden_states)\n",
    "\n",
    "print(f\"Input shape: {hidden_states.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6faf8-c0bb-4a92-9654-322f06483c9a",
   "metadata": {},
   "source": [
    "## Compare Our Output vs HF Output\n",
    "HF Outout uses default Vision Embeddings. \\\n",
    "Our Output uses our custom Vision Embeddings. \\\n",
    "There is also a mapping between hf_state_dict and our_state_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef8bed-fb8e-43b3-bda6-0e2bd448752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from transformers import SiglipVisionModel as HFSiglipVisionModel\n",
    "from transformers import SiglipVisionConfig as HFSiglipVisionConfig\n",
    "\n",
    "# HF output and HF State dictionary\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "hf_vision_model = HFSiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\", \n",
    "                                                      config=HFSiglipVisionConfig(vision_use_head=False))\n",
    "\n",
    "hf_state_dict = {k.replace(\"vision_model.embeddings.\", \"\"): v \\\n",
    "                 for k, v in hf_vision_model.state_dict().items() if \"vision_model.embeddings.\" in k}\n",
    "\n",
    "hf_vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745d053-e1e9-4f1e-ac44-49b623710398",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_state_dict = hf_vision_model.vision_model.state_dict()\n",
    "print(\"\\n --------all the key names in hf_state_dict ---------\\n\")\n",
    "for k, v in hf_state_dict.items() :\n",
    "    print(\"key:\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e02b60-232c-429f-9c9f-128e3d59d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_state_dict = attention.state_dict()\n",
    "print(\"\\n --------all the key names in our_state_dict ---------\\n\")\n",
    "for k, v in our_state_dict.items() :\n",
    "    print(\"key:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245897fe-eef3-4bd1-b9d7-cd632d7f42a6",
   "metadata": {},
   "source": [
    "## Sanity Check #2\n",
    "- We have not implemented the entire Siglip : Transformer Image encoder yet\n",
    "- Hence compare the output from our SiglipAttention vs HF Vision Model's Zeroth Encoder's multi head attention module\n",
    "\n",
    "If they match \n",
    "- it means that we were able to load the weights from Hugging Face Siglip's zero-th encoder layers's multi head attention module succesfully into our SiglipAttention module\n",
    "- it means that our def forward implementation of SiglipAttention is correct / same as the def forward of Hugging Face Siglip Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397fa33-9102-4c02-962d-c71d6ffcab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the key mapping between our model(on the left) and the Hugging face model keys on the right\n",
    "# Notice that our model just does not have the encoder.layers.0.self_attn. part in the name.\n",
    "# This is because we have not yet implemented the full encoder. We have only implemented the SiglipAttention\n",
    "key_mapping = {\n",
    "    'k_proj.weight': 'encoder.layers.0.self_attn.k_proj.weight',\n",
    "    'k_proj.bias': 'encoder.layers.0.self_attn.k_proj.bias',\n",
    "    'v_proj.weight': 'encoder.layers.0.self_attn.v_proj.weight',\n",
    "    'v_proj.bias': 'encoder.layers.0.self_attn.v_proj.bias',\n",
    "    'q_proj.weight': 'encoder.layers.0.self_attn.q_proj.weight',\n",
    "    'q_proj.bias': 'encoder.layers.0.self_attn.q_proj.bias',\n",
    "    'out_proj.weight': 'encoder.layers.0.self_attn.out_proj.weight',\n",
    "    'out_proj.bias': 'encoder.layers.0.self_attn.out_proj.bias'\n",
    "}\n",
    "\n",
    "for our_key, hf_key in key_mapping.items():\n",
    "    our_state_dict[our_key].copy_(hf_state_dict[hf_key])\n",
    "\n",
    "attention.load_state_dict(our_state_dict)\n",
    "\n",
    "with torch.no_grad():\n",
    "    our_output = attention(hidden_states)\n",
    "    hf_output = hf_vision_model.vision_model.encoder.layers[0].self_attn(hidden_states)[0]\n",
    "    max_diff = torch.max(torch.abs(our_output - hf_output))\n",
    "    print(f\"\\n Max difference between our output and HF output: {max_diff:.6f}\")\n",
    "    print((torch.isclose(our_output, hf_output, atol=1e-6)==0).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
