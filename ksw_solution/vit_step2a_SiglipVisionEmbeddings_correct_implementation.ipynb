{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1c2bdc-e858-4332-bbe6-46ad17d838e2",
   "metadata": {},
   "source": [
    "# vit_step2a_SiglipVisionEmbeddings_correct_implementation\n",
    "This is the correct implementation of SiglipVisionEmbeddings. \\\n",
    "See comparison of our-embeddings-output vs hugging-face-embeddings-output section for more insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c733290-d396-45b4-bdd6-d200e2738792",
   "metadata": {},
   "source": [
    "## Imports\n",
    "These are some preliminary imports that are needed. \\\n",
    "Some other imports for HF output are imported later to prevent confusion in understanding between HFSiglipVisionConfig vs our custom defined SiglipVisionConfig etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dce49-2efe-4625-b603-44b9a7a277bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a77241-90de-43a2-8843-4d8e3329336b",
   "metadata": {},
   "source": [
    "## Image + Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaa0ee-173f-4cf2-9d0e-1037aaab13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, image_size=224):\n",
    "    # define the preprocess operation\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # actually preprocess the image\n",
    "    image_tensor = preprocess(image)\n",
    "    #(3,224,224) --> unsqueeze -->(1,3,224,224)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "img = Image.open(\"image.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827d41d-d8fe-4a37-a984-7813faac160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = preprocess_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6574477-f54f-42b3-a567-d358211d476f",
   "metadata": {},
   "source": [
    "## SiglipVisionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a77aaf-c90f-471a-b0fc-06ade1e3aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@dataclass is a decorator in Python that automatically generates special methods like __init__, __repr__, and __eq__ for a class. \n",
    "This simplifies the creation of classes primarily used for storing data. \n",
    "It reduces boilerplate code and improves readability, especially when dealing with objects that mainly hold data.\n",
    "'''\n",
    "@dataclass\n",
    "class SiglipVisionConfig:\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    hidden_size: int = 768\n",
    "    num_channels: int = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a432050-e58e-475a-9b54-95fdad5db1a5",
   "metadata": {},
   "source": [
    "## SiglipVisionEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9b1d4-0f7b-4e5d-b18b-7c931274d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config    \n",
    "\n",
    "        self.image_size   = config.image_size\n",
    "        self.patch_size   = config.patch_size                \n",
    "        self.embed_dim   = config.hidden_size # same as embedding size\n",
    "        self.num_channels = config.num_channels\n",
    "    \n",
    "        # Patch embedding Layer: \n",
    "        # This is just a convolution layer of the patch size with kernel size same as patch size\n",
    "        # Example: For patch_size = 16, This is a convolution of kernel size: 16*16 and stride 16\n",
    "        # The input to this is an image tensor (see forward)\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels  = self.num_channels,\n",
    "            out_channels = self.embed_dim,\n",
    "            kernel_size  = self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding = \"valid\" # same as no padding\n",
    "        )\n",
    "    \n",
    "        # // is floor division. ** is exponentiation. \n",
    "        # You square by 2 because you have patches along the length and breadth\n",
    "        self.num_patches   = (self.image_size// self.patch_size)**2\n",
    "        self.num_positions = self.num_patches\n",
    "    \n",
    "        # Position Embedding Layer. This is a lookup table \n",
    "        # The input to this is a bunch of position_ids and not an image tensor, see forward        \n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "    \n",
    "        # Register Buffer/ Position ids\n",
    "        # registers a non-trainable buffer called position_ids in a nn.Module subclass (so this creates self.position_ids)\n",
    "        # self.position_ids, which will be a tensor of shape [1, num_patches]\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False, # this is a buffer, so it won't be updated during the forward pass\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        # num_batches, num_channels, height, width\n",
    "        B, C, H, W = pixel_values.shape\n",
    "        # Patch embeddings\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        flattened_patch_embeds = patch_embeds.flatten(start_dim=2, end_dim=-1)\n",
    "        flattened_patch_embeds = flattened_patch_embeds.transpose(1, 2)\n",
    "        # Position embeddings\n",
    "        position_embeds = self.position_embedding(self.position_ids)\n",
    "        # Total Embeddings\n",
    "        total_embeds = flattened_patch_embeds + position_embeds\n",
    "        return total_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a56d5-f57a-4926-9e24-7361cf479840",
   "metadata": {},
   "source": [
    "## Compare Our Output vs HF Output\n",
    "HF Outout uses default Vision Embeddings. \\\n",
    "Our Output uses our custom Vision Embeddings: SiglipVisionEmbeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84acb5c-1dca-415e-9476-eb154991bf21",
   "metadata": {},
   "source": [
    "### i) Sanity Check Our-Embeddings & Our-Embeddings-State-Dict\n",
    "- Check the our_embeddings output shape . It should have the dimensions torch.Size([1, 196, 768])\n",
    "- Also print the state_dict of our_embeddings. It should have the same keys as hf_state_dict.\n",
    "  \n",
    "our-embeddings-state-dict should have the following keys\n",
    "- key: patch_embedding.weight\n",
    "- key: patch_embedding.bias\n",
    "- key: position_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d582d-3e73-4020-aa68-bd175e0ec73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our embeddings\n",
    "our_embeds = SiglipVisionEmbeddings(SiglipVisionConfig())\n",
    "print(\"our_embed(image_tensor).shape:\", our_embeds(image_tensor).shape)\n",
    "\n",
    "# State Dictionary of our embeddings\n",
    "our_embeds_state_dict = our_embeds.state_dict()\n",
    "\n",
    "print(\"\\n--------our_embeds_state_dict: keys ---------\")\n",
    "for k, v in our_embeds_state_dict.items() :\n",
    "    print(\"key:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8347fb-1c24-4ad1-8ccf-4a9a5a18b046",
   "metadata": {},
   "source": [
    "### ii) Load Hugging Face : SiglipVisionModel . Observer the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750583ac-486e-4765-bd1b-3fe5f94aa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipVisionModel as HFSiglipVisionModel\n",
    "from transformers import SiglipVisionConfig as HFSiglipVisionConfig\n",
    "\n",
    "# HF output and HF State dictionary\n",
    "hf_vision_model = HFSiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\", \n",
    "                                                      config=HFSiglipVisionConfig(vision_use_head=False))\n",
    "hf_vision_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817c5ee-c3d3-4540-bbbb-9bde8d4446bb",
   "metadata": {},
   "source": [
    "### iii) Load Hugging Face : SiglipVisionModel's Entire State Dict. Observe all the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42265a-6684-4e03-8c8a-af2078d37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n --------hf_vision_model: all the key names in state_dict---------\\n\")\n",
    "for k, v in hf_vision_model.state_dict().items() :\n",
    "    print(\"key:\", k)\n",
    "        \n",
    "print(\"\\n\\n--------hf_vision_model: only the keys beginning with vision_model.embeddings ---------\")\n",
    "print(\"So this would be the hf_embeds_state_dict keys: before updating the key names \\n\")\n",
    "for k, v in hf_vision_model.state_dict().items() :\n",
    "    if \"vision_model.embeddings.\" in k:\n",
    "        print(\"key:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35841adb-f5f0-44ff-8a3c-b2b7f2d87585",
   "metadata": {},
   "source": [
    "### iv) Specifically only extract the embeddings values and dictionary from hugging face model.\n",
    "###     Update the hf_embeds_state_dict key names to be the same as our_embeds_state_dict\n",
    "Notice that we don't really care about the entire hf_vision_model's state dictionary. \\\n",
    "We only care about the subset of the state dictionary related to the embeddings. \\\n",
    "Hence it is more appropriate to call the dictionary hf_embeds_state_dict. \n",
    "\n",
    "Rename the following keys i.e. the prefix vision_model.embeddings will be dropped\n",
    "- vision_model.embeddings.patch_embedding.weight -> patch_embedding.weight\n",
    "- vision_model.embeddings.patch_embedding.bias -> patch_embedding.bias\n",
    "- vision_model.embeddings.position_embedding.weight -> position_embedding.weight \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1d0b5-3dc4-4fe8-a81c-f375948e8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hf_embeds_state_dict keys\n",
    "hf_embeds_state_dict = {k.replace(\"vision_model.embeddings.\", \"\"): v \\\n",
    "                 for k, v in hf_vision_model.state_dict().items() if \"vision_model.embeddings.\" in k}\n",
    "\n",
    "print(\"\\n\\n--------hf_embeds_state_dict keys: after updating their names ---------\\n\")\n",
    "for k, v in hf_embeds_state_dict.items() :\n",
    "    print(\"key:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297f79b-931a-42bb-83c6-f03c243abf1f",
   "metadata": {},
   "source": [
    "### v) Update our_embeds_state_dict with key & value pairs from hf_embeds_state_dict\n",
    "**does the following**\n",
    "- It updates the dictionary our_embeds_state_dict in-place with the key-value pairs from hf_embeds_state_dict.\n",
    "- So this effectively translates to loading the structure of the layers and their weights from hugging-face-embeddings to our embeddings implementation. This might seem supefluous. After all what is the point defining the conv2d layers and nn.Embedding etc.\n",
    "- The point is, doing this allows us the to check the **def forward** implementation of the SiglipEmbeddings. If sth is wrong there , then this would catch the wrong implementation.\n",
    "- See vit_step2b ... vit_ste2c, vit_step2d..._wrong_implementation to understand this better\n",
    "\n",
    "**So specifically**\n",
    "- our_embeds_state_dict is the state dict of the embedding layer from our custom model.\n",
    "- hf_embeds_state_dict is the state dict of the pretrained Hugging Face model embedding layer.\n",
    "\n",
    "**What happens**\n",
    "- Each key in hf_embeds_state_dict that matches or is new to our_embeds_state_dict will be added or **overwritten.**\n",
    "- This allows you to initialize or partially load **weights of only the embeddings** from the Hugging Face model into our own embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8db14-e24c-4e7a-9fd1-9cf700c9832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeds_state_dict.update(hf_embeds_state_dict)\n",
    "print(\"\\n--------our_embeds_state_dict: keys ---------\")\n",
    "for k, v in our_embeds_state_dict.items() :\n",
    "    print(\"key:\", k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd0167-5d72-4a7f-a635-4d3980903467",
   "metadata": {},
   "source": [
    "### vi) Update our-embeddings with our_embeds_state_dict (which has the hugging face weights for the embeddings only) \n",
    "Notice that our_embeds and our_embeds_state_dict are two different things\n",
    "- in the previous step we loaded the layers and weights from hugging-face-model-embeddings to our_embeds_state_dict\n",
    "- in this step we load the layers and weights from our_embeds_state_dict to our_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0683b-6748-405f-9d90-139e246d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeds.load_state_dict(our_embeds_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c59701-b996-4c52-8b8d-e0f01d554627",
   "metadata": {},
   "source": [
    "### vii) Final Step: Get the difference between the our-embeddings output and hugging-face-embeddings output\n",
    "Note: At this stage both our-embeddings and hugging-face-embeddings have the same weights i.e. the hugging face weights. \\\n",
    "What differs is really SiglipEmbeddings: def forward vs hugging-face-embeddings def forward. \\\n",
    "If our implementation is right the difference should be zero. See expected output below \n",
    "\n",
    "<div style=\"background-color:#C1FFC1; padding: 5px;\">\n",
    "# Expected Output   \n",
    "    \n",
    "Max difference between our output and HF output: tensor(0.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21106c3b-cccf-4da5-a1c9-36e8cd13ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between hf_output and our_output\n",
    "with torch.no_grad():\n",
    "    our_output = our_embeds(image_tensor)\n",
    "    hf_output = hf_vision_model.vision_model.embeddings(image_tensor)\n",
    "    print(\"Max difference between our output and HF output:\", torch.max(torch.abs(our_output - hf_output))) # =0, so they match!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
