{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e2789-f641-4add-a059-28a8bf3d2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Do this once. \n",
    "!curl -L http://i.imgur.com/8o9DXSj.jpeg --output image.jpg \n",
    "# Make sure to restart your runtime before running again\n",
    "!pip install transformers!pip install transformers[sentencepiece]\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install sentencepiece\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fd771-ca61-4b64-b179-caff1c376c00",
   "metadata": {},
   "source": [
    "## Import the Pretrained SiglipVisionModel from Hugging Face\n",
    "- This model will be imported as SiglipVisionModel\n",
    "- from_pretrained means the entire Model with pretrained weights from Hugging Face\n",
    "- \"google/siglip-base-patch16-224\": Is the model checkpoint. patch16 means 16x16 patches. 224 means it uses a 224x224 image as input\n",
    "\n",
    "Print the hf_vision_model at the end. It should have all the layers in the SIGLIP : VISION TRANSFORMER ARCHITECTURE DIAGRAM specified in the Readme.Md\n",
    "- **i) The Embeddings:** with Patch Embeddings and Position Embeddings\n",
    "- **ii) Encoder :** with 12x Single Encoder layers. Each Encoder layer will have layer_norm1, self_attention, layer_norm2, mlp. \\\n",
    "  Each self_attn(multi head attention) block wil have K, Q, V and a out_proj layer (the final linear layer after the concatenation). \\\n",
    "  Each MLP will have fc1, Gelu and fc2\n",
    "- **iii) Post Layer Norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ed716-bfa5-4c3c-a749-b3dc104c3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipVisionModel, SiglipVisionConfig\n",
    "model_checkpoint = \"google/siglip-base-patch16-224\"\n",
    "vision_model = SiglipVisionModel.from_pretrained(model_checkpoint, \n",
    "                                                 config=SiglipVisionConfig(vision_use_head=False))\n",
    "vision_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e3a19-d0d8-415f-b1a0-e2c7d961e146",
   "metadata": {},
   "source": [
    "## Input Image + Preprocess Image\n",
    "The model cannot accept the image as is. \n",
    "- It has to be resized to 224x224\n",
    "- It has to be converted to a tensor\n",
    "- It has to be normalized: these numbers come from the Imagenet dataset (industry standard)\n",
    "- Unsqueeze the tensor to include the batch dimension so that the transformer model can use it (in this case batch dimension is 1). (3,224,224) --> unsqueeze -->(1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f478065-6911-4c9f-b13b-7a318ef94bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"image.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10894e7-e790-4831-ae72-a1a85c5beeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_image(image, image_size=224):\n",
    "    # define the preprocess operation\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # actually preprocess the image\n",
    "    image_tensor = preprocess(image)\n",
    "    #(3,224,224) --> unsqueeze -->(1,3,224,224)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "image_tensor = preprocess_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236f05d-877f-4ed6-8940-e153a98ab78a",
   "metadata": {},
   "source": [
    "# Patches and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f255e-8690-40f5-adc4-f79d7bb51c76",
   "metadata": {},
   "source": [
    "## Patch embeddings\n",
    "details in Readme.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfd21c-631f-470b-829f-de300e8e4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding parameters. These are the parameters used by PaliGemma2 model\n",
    "# embed_dim means that each patch will be converted to a vector of dimension = 768 (that is the embedding output)\n",
    "embed_dim = 768 \n",
    "patch_size = 16\n",
    "image_size = 224\n",
    "num_patches = (image_size // patch_size)**2\n",
    "\n",
    "## Patch embedding\n",
    "with torch.no_grad():\n",
    "    '''\n",
    "    torch.no grad means we are not going to update the weights of the convolution filter.\n",
    "    A nn.conv2d filter with random weights has been created. \n",
    "    The patch embeddings using this filter will be calculated.\n",
    "    '''\n",
    "    # i) input = image_tensor\n",
    "    input = image_tensor\n",
    "    # ii) layer = patch_embedding_filter\n",
    "    # This is like a mini __init__\n",
    "    patch_embedding_filter = nn.Conv2d(in_channels =3,\n",
    "                                out_channels= embed_dim,\n",
    "                                kernel_size = patch_size,\n",
    "                                stride = patch_size)\n",
    "    # iii) output = patch_embeddings\n",
    "    # This one is a like a mini forward    \n",
    "    patch_embeddings = patch_embedding_filter(input)\n",
    "\n",
    "# Flatten the patches\n",
    "# After flattening (1, embed_dim , num_patches) = (1,768,196)\n",
    "flattened_patch_embeddings = patch_embeddings.flatten(start_dim =2, end_dim =-1)\n",
    "# (1,768,196) -> (1,196,768) = (1, num_patches, embed_dim) \n",
    "flattened_patch_embeddings = flattened_patch_embeddings.transpose(1,2)\n",
    "\n",
    "print(\"------ PATCH EMBEDDINGS -------\")\n",
    "print(\" The following would show there are 14 patches on the height & 14 on the width.\\\n",
    "Each patch has been converted to a vector of 768. \\\n",
    "Total number of patches = 14x14 = 196 \\n\")\n",
    "print(\"num_patches   =\", num_patches)\n",
    "print(\"i)   input : image_tensor.shape : \", image_tensor.shape)\n",
    "print(\"ii)  layer : patch_embedding_filter  :\", patch_embedding_filter)\n",
    "print(\"iii) output: patch_embeddings.shape : \", patch_embeddings.shape)\n",
    "print(\"iii) output: flattened_patch_embeddings.shape : \", flattened_patch_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee1f01-340e-486b-9ad0-4ba4a08b45cc",
   "metadata": {},
   "source": [
    "## Position Embeddings\n",
    "details in Readme.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33201c-0c45-47a8-a5f6-513138ef5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Position Embeddings\n",
    "'''\n",
    "Find out why torch.no_grad() is not used here ?. \n",
    "'''\n",
    "\n",
    "# i) input = position_ids. \n",
    "# Notice that there is no image_tensor involved for position_embeddings. \n",
    "# Its just a lookup based on input position ids.\n",
    "# the expand((1,-1)) just means expand it by the batch dimension so that the transformer can use it\n",
    "position_ids = torch.arange(num_patches).expand((1,-1))\n",
    "input = position_ids\n",
    "# ii) layer = position_embedding_lookup \n",
    "position_embedding_lookup = nn.Embedding(num_patches, embed_dim)\n",
    "# iii) output = position_embeddings\n",
    "position_embeddings = position_embedding_lookup(position_ids)\n",
    "\n",
    "print(\"\\n------ POSITION EMBEDDINGS -------\")\n",
    "print(\"i)   input : position_ids.shape : \", position_ids.shape)\n",
    "print(\"ii)  layer : position_embedding_lookup :\", position_embedding_lookup)\n",
    "print(\"iii) output: position_embeddings.shape : \", position_embeddings.shape)\n",
    "print(\"\\n\")\n",
    "print(\"i)   input : the list of position_ids \\n\", position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c1990-b798-4165-80bf-b0f995dc0275",
   "metadata": {},
   "source": [
    "## Total Embeddings (patch & position embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2684b-76ba-454d-93b5-bbbafe731643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total Embeddings\n",
    "embeddings = flattened_patch_embeddings + position_embeddings\n",
    "print(\"\\n------ BOTH EMBEDDINGS : patch and position embeddings -------\")\n",
    "print(\"embeddings.shape :\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78668326-bc00-4b0a-9409-b7394549e4cc",
   "metadata": {},
   "source": [
    "## Visualize Embeddings : Before Training\n",
    "details in Readme.md\n",
    "\n",
    "embeddings[0]: [0] refers to the batch dimension and likely the first image in the batch. Since there is only one image in the batch, it would be [0] index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485d66e-a940-40ed-9879-61afd8bf8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings(embeds_viz, title):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(embeds_viz, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Embedding dimension')\n",
    "    plt.ylabel('Patch number')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the embeddings before training \n",
    "print(\"Flattened Patch Embeddings: Before Training\")\n",
    "print(\"They should look all random, since the weights of the conv2d filter are random at the initialization\")\n",
    "embeds_viz = flattened_patch_embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"flattened_embeds_viz.shape =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Flattened Patch Embeddings: Before Training\")\n",
    "\n",
    "print(\"\\n\\nPosition Embeddings: Before Training\")\n",
    "print(\"They should look all random, since the weights of nn.Embedding lookup are random at the initialization\")\n",
    "embeds_viz = position_embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"position_embeds_viz.shape =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Position Embeddings: Before Training\")\n",
    "\n",
    "print(\"\\n\\nEmbeddings(both, flattened_patch + position): Before Training\")\n",
    "print(\"They should look all random, since the weights are random at the initialization. \\n\\\n",
    "Notice total_embeddings look pretty similar to patch_embeddings, despite adding position_embeddings.\\n\\\n",
    "This is likely because position_embeddings are supposed to be small displacement vectors. \\n\\\n",
    "And, that the change they have caused is not visible in such a visualization.\")\n",
    "embeds_viz = embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"total_embeds_viz.shape =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Embeddings(both,flattened_patch + position): Before Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6072d8-f367-4fbf-a0c2-ae673060df14",
   "metadata": {},
   "source": [
    "## Visualize Embeddings : After Training\n",
    "- Its not that the  nn.Conv2d Filter , and nn.Embedding lookup table used to create untrained patch and position embeddings respectively have been trained. i.e. there is no model training or embeddings training step in **vit_step1_img_prepocess_embeddings.ipynb**\n",
    "- Instead download the pre-trained SiglipVisionModel from Hugging face . Visualize the trained embeddings from this model.\n",
    "- details in Readme.md\n",
    "\n",
    "trained_total_embeddings[0]: [0] refers to the batch dimension and likely the first image in the batch. Since there is only one image in the batch, it would be [0] index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fd73b-45f8-4b80-a1b3-4d33fbc57fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "vision_model.eval()\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    trained_patch_embeddings = vision_model.vision_model.embeddings.patch_embedding(inputs.pixel_values)    \n",
    "    trained_position_embeddings = vision_model.vision_model.embeddings.position_embedding(position_ids)\n",
    "    trained_total_embeddings = vision_model.vision_model.embeddings(inputs.pixel_values)\n",
    "\n",
    "\n",
    "# Flatten the patches\n",
    "# After flattening (1, embed_dim , num_patches, embed_dim) = (1,196, 768)\n",
    "# Note: no need to transpose these (like the before training ones). These come out transposed\n",
    "trained_flattened_patch_embeddings = trained_patch_embeddings.flatten(start_dim =2, end_dim =-1)\n",
    "\n",
    "print(\"trained_patch_embeddings.shape           : \", trained_patch_embeddings.shape)\n",
    "print(\"trained_flattened_patch_embeddings.shape : \", trained_flattened_patch_embeddings.shape)\n",
    "print(\"trained_position_embeddings.shape        : \", trained_position_embeddings.shape)\n",
    "print(\"trained_total_embeddings.shape           : \", trained_total_embeddings.shape)\n",
    "\n",
    "# Visualize the embeddings after training \n",
    "print(\"\\n\\nFlattened Patch Embeddings: After Training i.e. from pretrained Hugging Face SiglipVision model\")\n",
    "embeds_viz = trained_flattened_patch_embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"flattened_embeds_viz.shape =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Flattened Patch Embeddings: After Training\")\n",
    "\n",
    "print(\"\\n\\nPosition Embeddings: After Training i.e. from pretrained Hugging Face SiglipVision model\")\n",
    "embeds_viz = trained_position_embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"position_embeds_viz.shape =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Position Embeddings: After Training\")\n",
    "\n",
    "print(\"\\n\\nEmbeddings(both, flattened_patch + position): After Training i.e. from pretrained Hugging Face SiglipVision model\")\n",
    "embeds_viz = trained_total_embeddings[0].detach().numpy() #shape: [196, 768]\n",
    "print(\"embeds_viz =\", embeds_viz.shape)\n",
    "visualize_embeddings(embeds_viz, \"Embeddings(both,flattened_patch + position): After Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
