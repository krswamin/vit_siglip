{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40e6af4-d52b-4ba1-96e4-2a667dd1a87c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "These are some preliminary imports that are needed. \\\n",
    "Some other imports for HF output are imported later to prevent confusion in understanding between HFSiglipVisionConfig vs our custom defined SiglipVisionConfig etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39872a51-3bc5-42ad-ae69-6858510f7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e0e24-892c-4886-9ac6-86b81fd0f801",
   "metadata": {},
   "source": [
    "## Input Image + Preprocess Image\n",
    "The model cannot accept the image as is. \n",
    "- It has to be resized to 224x224\n",
    "- It has to be converted to a tensor\n",
    "- It has to be normalized: these numbers come from the Imagenet dataset (industry standard)\n",
    "- Unsqueeze the tensor to include the batch dimension so that the transformer model can use it (in this case batch dimension is 1). (3,224,224) --> unsqueeze -->(1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa5d19-0131-42a4-b828-9390f0b6101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, image_size=224):\n",
    "    # image_size is the size to which the image will be resized\n",
    "    # define the preprocess operation\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # actually preprocess the image\n",
    "    image_tensor = preprocess(image)\n",
    "    #(3,224,224) --> unsqueeze to include batch dimension -->(1,3,224,224)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    return image_tensor\n",
    "\n",
    "img = Image.open(\"image.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048cb2d2-0e67-4b0b-a6fd-2d733b667ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = preprocess_image(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8add6bd8-b2f4-462f-a006-10e6d90d608c",
   "metadata": {},
   "source": [
    "## Attention Formula , Single Head of Attention, Multi Head Attention\n",
    "See Readme.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b032a5-bf34-4689-b618-4ca7d4e977f0",
   "metadata": {},
   "source": [
    "## Building Block 1: SiglipVisionConfig\n",
    "The config values are the ones that the language model PaliGemma2 uses.\n",
    "- image_size (original input image ) = 224*224 ( you would have to preprocess the image to change to this size. See preprocess_image\n",
    "- patch_size = 16. So each patch will be 16 x 16 pixels.\n",
    "- embedding_size = hidden_size = 768. This means every image patch will be converted to a vector of 768 dimension. \n",
    "- each image will have 224/16 = 14 patches in every row and every column. So total_num_patches= 14*14 = 196. Each of the 196 patches will be converted to a vector of dimension 768. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535cad2-af5c-4347-a475-43ab65aed9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@dataclass is a decorator in Python that automatically generates special methods like __init__, __repr__, and __eq__ for a class. \n",
    "This simplifies the creation of classes primarily used for storing data. \n",
    "It reduces boilerplate code and improves readability, especially when dealing with objects that mainly hold data.\n",
    "'''\n",
    "@dataclass\n",
    "class SiglipVisionConfig:   \n",
    "    image_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    hidden_size: int = 768 # same as embedding size\n",
    "    intermediate_size: int = 3072\n",
    "    num_channels: int = 3\n",
    "    num_attention_heads: int = 12   \n",
    "    num_hidden_layers: int = 12 # number of hidden/encoder layers in the encoder as in the paper\n",
    "    attention_dropout: float = 0.0\n",
    "    layer_norm_eps: float = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38803344-4396-4cc9-b249-aec58edd0080",
   "metadata": {},
   "source": [
    "## Building Block 2: SiglipVisionEmbeddings\n",
    "The image is coverted to embeddings. \\\n",
    "All the information in the image is captured by embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccb6b5-af50-484b-b877-f4d7f4c0220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config    \n",
    "\n",
    "        self.image_size   = config.image_size\n",
    "        self.patch_size   = config.patch_size                \n",
    "        self.embed_dim   = config.hidden_size # same as embedding size\n",
    "        self.num_channels = config.num_channels\n",
    "    \n",
    "        # Patch embedding Layer: \n",
    "        # This is just a convolution layer of the patch size with kernel size same as patch size\n",
    "        # Example: For patch_size = 16, This is a convolution of kernel size: 16*16 and stride 16\n",
    "        # The input to this is an image tensor (see forward)\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels  = self.num_channels,\n",
    "            out_channels = self.embed_dim,\n",
    "            kernel_size  = self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding = \"valid\" # same as no padding\n",
    "        )\n",
    "    \n",
    "        # // is floor division. ** is exponentiation. \n",
    "        # You square by 2 because you have patches along the length and breadth\n",
    "        self.num_patches   = (self.image_size// self.patch_size)**2\n",
    "        self.num_positions = self.num_patches\n",
    "    \n",
    "        # Position Embedding Layer. This is a lookup table \n",
    "        # The input to this is a bunch of position_ids and not an image tensor, see forward        \n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "    \n",
    "        # Register Buffer/ Position ids\n",
    "        # registers a non-trainable buffer called position_ids in a nn.Module subclass (so this creates self.position_ids)\n",
    "        # self.position_ids, which will be a tensor of shape [1, num_patches]\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False, # this is a buffer, so it won't be updated during the forward pass\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        # num_batches, num_channels, height, width\n",
    "        B, C, H, W = pixel_values.shape\n",
    "        # Patch embeddings\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        flattened_patch_embeds = patch_embeds.flatten(start_dim=2, end_dim=-1)\n",
    "        flattened_patch_embeds = flattened_patch_embeds.transpose(1, 2)\n",
    "        # Position embeddings\n",
    "        position_embeds = self.position_embedding(self.position_ids)\n",
    "        # Total Embeddings\n",
    "        total_embeds = flattened_patch_embeds + position_embeds\n",
    "        return total_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df134c60-067a-4019-9c47-708d30b17e17",
   "metadata": {},
   "source": [
    "## Building Block 3: SiglipAttention\n",
    "- Vectorized implementation of Multi head attention. Same implementation as Hugging Face (different from the non vectorized implementation in vit_step3 Head and MultiHeadAttention classes)\n",
    "- You don't have Single Attention Heads. You process all single attention heads parallely\n",
    "- This is more memory efficient\n",
    "- This enables using Hugging Face's pretrained weights in our model\n",
    "\n",
    "#### Vectorized Implementation\n",
    "In the vectorized implementation there might seem to be a lot of transposes, shape changes etc. \\\n",
    "Here is a high leve overview of it. \n",
    "- i)   q, k, v_states are the same dimensions as hidden_states = [1,196,768] = [batch, num_patches, embedding_dimension] \n",
    "- ii)  split q,k,v_states into 12 attention heads, along the embedding_dimension (768)  = [1,196,12,64]\n",
    "- iii) transpose q,k, v_states so that its [batch, num_heads] first and then [196,64] for vectorized multiplication\n",
    "- iv)  attention filter = q*k_transpose is a square: [1,12, 196,196]\n",
    "- v)   scaled attention filter, softmax, dropout are all the same square = [1,12,196,196]\n",
    "- vi)  mutltiply attention_weights*v and the dimesion is back to : [ 1, 12, 196,64]\n",
    "- vii) transpose attention again so that [12,64] the embeddind dimensions are back together for the concatenation: [ 1,196,12,64]\n",
    "- viii) Concatenate: merge 12 attention heads to get back the embedding dimension 768=12*64: [1. 196, 768]\n",
    "- ix)   Project back to residual states shape(which happens to be the same): [1,196,768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46976c3-34d1-476d-aaa7-fbb8e1ef7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipAttention(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.dropout = config.attention_dropout\n",
    "        \n",
    "        # The linear layers whose outputs will result in query, key, value respectively\n",
    "        # This is just one unified set of projection heads across the the multi head attention module\n",
    "        # i.e there arent multiple projection layers defined for the single attention heads\n",
    "        # The output of this will be reshaped (see def forward). This is more memory efficient and\n",
    "        # enables the use of hugging face weights\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)   \n",
    "        \n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    # Note: the hidden states are the embeddings\n",
    "    def forward(self, hidden_states):\n",
    "        # the hidden states are the embeddings of the patches, so (batch_size, num_patches, embed_dim)\n",
    "        # Here B: batch_size = 1, T: num_tokens=num_patches = 196, C: embedding_dimension = 768\n",
    "        B, T, C = hidden_states.shape\n",
    "        \n",
    "        # Query, Key and Values\n",
    "        # These are the q, k,v values across all attention heads. There are 3 large vectors\n",
    "        # vectorized dot-product(mat-mul) of hidden_states =[1, 196, 768] & q_proj = [768,768]\n",
    "        # q,k,v_states: shape = [196x768]*[768x768] = [ 196x768] across all batches = [1,196,768]\n",
    "        q_states = self.q_proj(hidden_states)        \n",
    "        k_states = self.k_proj(hidden_states)    \n",
    "        v_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape Multi Head Attention into 12 units        \n",
    "        # Divide the Q, K, V vectors across all attention heads. This is what C // self.num_attention_heads does\n",
    "        # We split along the embedding dimension 768 (and not along the number of patches = 196)\n",
    "        # Hence qualitatively this is like splitting the large embedding vector into smaller pieces corresponding to the 12 individual heads\n",
    "        # This step is what we do differently: We do not concatenate anymore. \n",
    "        # i.e. in the previous step it was already concatenated and at 768. In this step we split 768 into 12*64 \n",
    "        # so q,k,v_states: ([1, 196, 768]) --> becomes torch.Size([1, 196, 12, 64])\n",
    "        q_states = q_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "        k_states = k_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "        v_states = v_states.view(B, T, self.num_heads, C // self.num_heads)\n",
    "\n",
    "        # Transpose the states so that the dot product dimensions i,e [196,64] are the last 2.\n",
    "        # first 2 dimensions are batch_size and num_heads. This enables easy vectorized mat-mul      \n",
    "        # q_states.transpose(1, 2) swaps dimension-index-1 with dimension-index-2 i.e. 2nd and 3rd dimension\n",
    "        # so q,k,v_states [1,196,12,64] ---> become [1,12,196,64]  \n",
    "        q_states = q_states.transpose(1, 2)\n",
    "        k_states = k_states.transpose(1, 2)\n",
    "        v_states = v_states.transpose(1, 2)\n",
    "\n",
    "        # The scale dk = 64\n",
    "        dk = k_states.size(-1)\n",
    "\n",
    "        # Attention-Filter = Q*Ktranspose (this is a square)\n",
    "        # k_states.transpose(-2, -1) swaps dimension-index-2 with dimension-index-1 i.e. last two dimensions\n",
    "        # vectorized dot-product(mat-mul) of q=[1,12,196,64] & k_transpose = [1,12,64,196]\n",
    "        # attn: shape = [196x64]*[64x196] = [ 196x196] across all 12 heads = [1,12,196,196]       \n",
    "        attn = q_states @ k_states.transpose(-2, -1)\n",
    "\n",
    "        # Scaled attention-filter: attn shape = [1,12,196,196]  \n",
    "        attn = attn/math.sqrt(dk)\n",
    "\n",
    "        # Apply softmax to get probability distribution: attn shape = [1,12,196,196]\n",
    "        # Since attn is [196, 196] it makes no sense to normalize for the entire square \n",
    "        # Apply softmax  along dimension -1 = dimension 1. This means \"across the columns\" i.e. rowwise\n",
    "        attn = F.softmax(attn, dim=-1).to(q_states.dtype)\n",
    "\n",
    "        # Dropout : attn shape = = [1,12,196,196]\n",
    "        # Why is dropout being applied before multiplication by v_states\n",
    "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Weighted Sum: allows information flow between tokens(patches)\n",
    "        # vectorized dot-product(mat-mul) of attn=[1,12,196,196] & v_states = [1,12,196,64]\n",
    "        # attn: shape = [196x196]*[196x64] = [ 196x64] across all 12 heads = [1,12,196,64]\n",
    "        attn = attn @ v_states\n",
    "\n",
    "        # Transpose it back to the original q_states view where [12,64] are the end\n",
    "        # so that they can be fused to make the original 12*64 = 768\n",
    "        # attn.transpose(1, 2) swaps dimension-index-1 with dimension-index-2 i.e. 2nd and 3rd dimension\n",
    "        # attn [1,12,196,64] ---> becomes [1,196,12,64]\n",
    "        attn = attn.transpose(1, 2)\n",
    "\n",
    "        # i) This reshaping concatenates the 12*64 outputs from 12 attention heads back to the embedding_dim = 768\n",
    "        # ii) Operations like transpose can make memory layout very inefficient.\n",
    "        #     Make sure that the vector is contiguous in memory for efficient implementation\n",
    "        # attn [1,196,12,64] ---> becomes [1,196,768]\n",
    "        attn = attn.reshape(B, T, C).contiguous()\n",
    "        \n",
    "        # The Final Linear Layer: To project to the desired output size/space\n",
    "        # vectorized dot-product(mat-mul) of attn =[1, 196, 768] & out_proj = [768,768]\n",
    "        # attn: shape = [196x768]*[768x768] = [ 196x768] across all batches = [1,196,768]\n",
    "        attn = self.out_proj(attn)\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9517a0-8195-4c40-bd1c-0565bab7b8da",
   "metadata": {},
   "source": [
    "## Building Block 4: SiglipMLP - Multi Layer Perception\n",
    "This is just a bunch of linear layers to map the hidden state to some other output dimension. \\\n",
    "Since the intermediate size is quite large at 3072 , you can learn more complex relations , at a higher dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca01ed-f66c-4500-8c34-fb3aae70b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipMLP(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "mlp = SiglipMLP(SiglipVisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "mlp(torch.randn(1, 196, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b408dc-338b-471f-876a-96992f48fc42",
   "metadata": {},
   "source": [
    "## Building Block 5: SiglipEncoderLayer\n",
    "Single Layer of the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653f48d-1d47-4a69-aa9a-e60e0bbf0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size        \n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.self_attn = SiglipAttention(config)        \n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = SiglipMLP(config)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states\n",
    "\n",
    "encoder_layer = SiglipEncoderLayer(SiglipVisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "encoder_layer(torch.randn(1, 196, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f8497-bc91-4342-b2d6-c1f10aebf1d2",
   "metadata": {},
   "source": [
    "## Building Block 6: SiglipEncoder\n",
    "The Encoder with many SiglipEncoderLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beacdbd-90b8-4b07-a024-8ac3f6937d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipEncoder(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for encoder_layer in self.layers:\n",
    "            hidden_states = encoder_layer(hidden_states)\n",
    "            \n",
    "        # Adding this for better readability and understanding for first timers\n",
    "        last_hidden_states = hidden_states\n",
    "        return last_hidden_states\n",
    "\n",
    "encoder = SiglipEncoder(SiglipVisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "encoder(torch.randn(1, 196, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96562dfe-ebd2-41d2-b063-f23a58900c7e",
   "metadata": {},
   "source": [
    "## Building Block 7: SiglipVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70885da-a79f-46fe-a0a4-a6fdc1b444d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        last_hidden_states = self.encoder(hidden_states)\n",
    "        last_hidden_states = self.post_layernorm(last_hidden_states)\n",
    "        return last_hidden_states\n",
    "\n",
    "our_siglip_transformer = SiglipVisionTransformer(SiglipVisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "our_siglip_transformer(image_tensor).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec5f6c-a5bd-44ab-8ba5-230863a55ff7",
   "metadata": {},
   "source": [
    "## The Grand Finale: SiglipVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d28e3-a736-49c1-ab88-dfb0ab04ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionTransformer(config)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        return self.vision_model(pixel_values)\n",
    "\n",
    "our_siglip_model = SiglipVisionModel(SiglipVisionConfig(hidden_size=768, intermediate_size=3072))\n",
    "our_siglip_model(image_tensor).shape\n",
    "\n",
    "our_siglip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637b081-4c30-419a-916f-e3e33cd9f2d8",
   "metadata": {},
   "source": [
    "## Import the Pretrained SiglipVisionModel from Hugging Face\n",
    "- This model will be imported as HFSiglipVisionModel\n",
    "- from_pretrained means the entire Model with pretrained weights from Hugging Face\n",
    "- \"google/siglip-base-patch16-224\": Is the model checkpoint. patch16 means 16x16 patches. 224 means it uses a 224x224 image as input\n",
    "\n",
    "Print the hf_vision_model at the end. It should have all the layers in the SIGLIP : VISION TRANSFORMER ARCHITECTURE DIAGRAM specified in the Readme.Md\n",
    "- **i) The Embeddings:** with Patch Embeddings and Position Embeddings\n",
    "- **ii) Encoder :** with 12x Single Encoder layers. Each Encoder layer will have layer_norm1, self_attention, layer_norm2, mlp. \\\n",
    "  Each self_attn(multi head attention) block wil have K, Q, V and a out_proj layer (the final linear layer after the concatenation). \\\n",
    "  Each MLP will have fc1, Gelu and fc2\n",
    "- **iii) Post Layer Norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef8bed-fb8e-43b3-bda6-0e2bd448752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipVisionModel as HFSiglipVisionModel\n",
    "from transformers import SiglipVisionConfig as HFSiglipVisionConfig\n",
    "\n",
    "# HF output and HF State dictionary\n",
    "hf_vision_model = HFSiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\", \n",
    "                                                    config=HFSiglipVisionConfig(vision_use_head=False))\n",
    "hf_vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9e262-c048-4de9-bbe1-e4f3e4646146",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_state_dict = hf_vision_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6faf8-c0bb-4a92-9654-322f06483c9a",
   "metadata": {},
   "source": [
    "## Compare Our Output vs HF Output\n",
    "As in we are not really comparing the output. We are only checking if the keys in the original Hugging Face Vision Model and the keys in the Vision Model that we defined match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3023db-b953-4745-80d4-2be2928e7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_state_dict = our_siglip_model.state_dict()\n",
    "our_siglip_model.load_state_dict(hf_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
